{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mingu/OneDrive/바탕 화면/성균관대/리서치인턴/공부/RL_study/dreamer\n"
     ]
    }
   ],
   "source": [
    "%cd RL_study/dreamer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 02:34:25.048968: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-04 02:34:25.070058: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-04 02:34:25.099153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-04 02:34:25.127906: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-04 02:34:25.134998: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-04 02:34:25.166346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-04 02:34:25.888296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "\n",
    "from tqdm import tqdm\n",
    "from models import *\n",
    "from logger import Logger\n",
    "\n",
    "\n",
    "env = gym.make('CarRacing-v2')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space: 3, obs shape: (96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "action_dim = env.action_space.shape[0]\n",
    "obs_shape = env.observation_space.shape\n",
    "print(\"action space: \",action_dim,\", obs shape: \", obs_shape,sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env,state_dim, transition_representation, agent,replay_buffer, num_episode, device, training=True):\n",
    "    print(\"collecting data...\")\n",
    "    score=0\n",
    "    for _ in tqdm(range(num_episode)):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        experience = []\n",
    "        prev_state = torch.zeros(1, state_dim).to(device)\n",
    "        prev_deter = transition_representation.init_hidden(1).to(device)\n",
    "        prev_action = torch.zeros(1, action_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            while not done:\n",
    "                #obs(96x96x3) -> (3x96x96) -> (1x3x96x96)\n",
    "                obs = torch.tensor(obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0).to(device)/255\n",
    "                # s_t-1, a_t-1, o_t-1 -> s_t\n",
    "                posterior_mean, posterior_std, prev_deter = transition_representation.posterior(prev_state, prev_action, prev_deter,obs)\n",
    "                cur_state = posterior_mean + posterior_std*torch.normal(0, 1, posterior_mean.size()).to(device)\n",
    "\n",
    "                action_mu, action_std = agent(cur_state, prev_deter)\n",
    "                eps = torch.normal(0, 1, (1,action_dim)).to(device)\n",
    "                if training:\n",
    "                    cur_action = torch.tanh(action_mu + action_std*eps)\n",
    "                else:\n",
    "                    cur_action = torch.tanh(action_mu)\n",
    "                next_obs, reward, terminated, truncated, info  = env.step(cur_action[0].cpu().numpy())\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                experience.append((np.array(obs.squeeze(0).cpu()), np.array(cur_action.squeeze(0).detach().cpu()), reward, done))\n",
    "                \n",
    "                obs = next_obs\n",
    "                prev_state = cur_state\n",
    "                prev_action = cur_action\n",
    "                score+=reward\n",
    "        if training:\n",
    "            for exp in experience:\n",
    "                replay_buffer.push(exp)\n",
    "    return score/num_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_return(rewards, values, gamma, lambda_):\n",
    "    # rewards, values : (Horizon+1, seq*batch)\n",
    "    # 어렵다\n",
    "    V_lambda = torch.zeros_like(rewards, device=rewards.device)\n",
    "\n",
    "    H = rewards.shape[0] - 1\n",
    "    V_n = torch.zeros_like(rewards, device=rewards.device)\n",
    "    V_n[H] = values[H]\n",
    "    for n in range(1, H+1):\n",
    "        # n-step 계산 하기 위함\n",
    "        # 각 step의 value 목표\n",
    "        V_n[:-n] = (gamma ** n) * values[n:]\n",
    "        for k in range(1, n+1):\n",
    "            # n step의 reward 합 진행\n",
    "            if k == n:\n",
    "                V_n[:-n] += (gamma ** (n-1)) * rewards[k:]\n",
    "            else:\n",
    "                V_n[:-n] += (gamma ** (k-1)) * rewards[k:-n+k]\n",
    "\n",
    "        # add lambda_ weighted n-step target to compute lambda target\n",
    "        if n == H:\n",
    "            V_lambda += (lambda_ ** (H-1)) * V_n\n",
    "        else:\n",
    "            V_lambda += (1 - lambda_) * (lambda_ ** (n-1)) * V_n\n",
    "            \n",
    "    return V_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch,state_dim,deterministic_dim, device, transition_representation, reward_model, observation, actor, value, model_optimizer, actor_optimizer, critic_optimizer):\n",
    "    obs_seq = []\n",
    "    action_seq = []\n",
    "    reward_seq = []\n",
    "    #batch = batch, seq, (obs, action, reward, done)\n",
    "    for seq in batch:\n",
    "        obs_temp=[]\n",
    "        action_temp=[]\n",
    "        reward_temp=[]\n",
    "        for (obs, action, reward, done) in seq:\n",
    "            obs_temp.append(obs)\n",
    "            action_temp.append(action)\n",
    "            reward_temp.append(reward)\n",
    "        obs_seq.append(obs_temp)\n",
    "        action_seq.append(action_temp)\n",
    "        reward_seq.append(reward_temp)\n",
    "    obs_seq = torch.tensor(obs_seq, dtype=torch.float32).to(device)\n",
    "    action_seq = torch.tensor(action_seq, dtype=torch.float32).to(device)\n",
    "    reward_seq = torch.tensor(reward_seq, dtype=torch.float32).to(device)\n",
    "    batch_size, seq_len, _, _, _ = obs_seq.size()\n",
    "    \n",
    "    prev_deter = transition_representation.init_hidden(batch_size).to(device)\n",
    "    prev_state = torch.zeros(batch_size, state_dim).to(device)\n",
    "    \n",
    "    states = torch.zeros(seq_len,batch_size, state_dim).to(device)\n",
    "    deters = torch.zeros(seq_len,batch_size, deterministic_dim).to(device)\n",
    "    \n",
    "    beta=0.1 #kl조절\n",
    "    imagine_horizon=15\n",
    "    gamma=0.99\n",
    "    lambda_=0.95\n",
    "    kl_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    reward_loss = 0\n",
    "    \n",
    "    total_kl_loss = 0\n",
    "    total_reconstruction_loss = 0\n",
    "    total_reward_loss = 0\n",
    "    \n",
    "    action_prev = action_seq[:, 0].to(device)\n",
    "    total_loss=torch.zeros(1).to(device)\n",
    "    for t in range(1,seq_len):\n",
    "        obs = obs_seq[:, t].to(device)\n",
    "        action = action_seq[:, t].to(device)\n",
    "        reward = reward_seq[:, t].to(device)\n",
    "        prior_mean, prior_std, _ = transition_representation(prev_state, action_prev, prev_deter)\n",
    "        posterior_mean, posterior_std, cur_deter = transition_representation.posterior(prev_state, action_prev, prev_deter,obs)\n",
    "        \n",
    "        state = posterior_mean + posterior_std*torch.normal(0, 1, posterior_mean.size()).to(device)\n",
    "        obs_pred = observation(state, cur_deter)\n",
    "        reconstruction_loss = nn.functional.mse_loss(obs_pred, obs)\n",
    "        \n",
    "        \n",
    "        reward_pred = reward_model(state, cur_deter)\n",
    "        reward_loss = nn.functional.mse_loss(reward_pred, reward)\n",
    "        \n",
    "        prior = Normal(prior_mean, prior_std)\n",
    "        posterior = Normal(posterior_mean, posterior_std)\n",
    "        kl_loss = kl_divergence(posterior, prior).mean()\n",
    "\n",
    "        \n",
    "        total_loss += reconstruction_loss + reward_loss + beta*kl_loss\n",
    "\n",
    "        action_prev = action\n",
    "        prev_state = state\n",
    "        prev_deter = cur_deter\n",
    "        \n",
    "        states[t] = state\n",
    "        deters[t] = cur_deter\n",
    "        \n",
    "        total_kl_loss += kl_loss.item()\n",
    "        total_reconstruction_loss += reconstruction_loss.item()\n",
    "        total_reward_loss += reward_loss.item()\n",
    "    model_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    model_optimizer.step()\n",
    "\n",
    "    \n",
    "    ##actor, critic 학습\n",
    "    \n",
    "    print(\"training actor, critic...\")\n",
    "    #states (seq, batch, state_dim) -> (seq*batch, state_dim)\n",
    "    #deters (seq, batch, deterministic_dim) -> (seq*batch, deterministic_dim)\n",
    "    states = states.view(-1, state_dim).detach()\n",
    "    deters = deters.view(-1, deterministic_dim).detach()\n",
    "    \n",
    "    imagined_states = [states]\n",
    "    imagined_deters = [deters]\n",
    "    \n",
    "    rewards = []\n",
    "    values = []\n",
    "    \n",
    "    \n",
    "    rewards.append(reward_model(states, deters).squeeze())\n",
    "    values.append(value(states, deters).squeeze())\n",
    "    \n",
    "    for t in range(1,imagine_horizon+1):\n",
    "        action_mu, action_std = actor(imagined_states[t-1], imagined_deters[t-1])\n",
    "        eps = torch.normal(0, 1, (action_mu.size())).to(device)\n",
    "        action = torch.tanh(action_mu + action_std*eps)\n",
    "        \n",
    "        prior_mean, prior_std, deter = transition_representation(imagined_states[t-1], action, imagined_deters[t-1])\n",
    "        state = prior_mean + prior_std*torch.normal(0, 1, prior_mean.size()).to(device)\n",
    "        \n",
    "        imagined_states.append(state)\n",
    "        imagined_deters.append(deter)\n",
    "        \n",
    "        rewards.append(reward_model(imagined_states[t], imagined_deters[t]).squeeze())\n",
    "        values.append( value(imagined_states[t], imagined_deters[t]).squeeze())\n",
    "    \n",
    "    imagined_states = torch.stack(imagined_states, dim=0)\n",
    "    imagined_deters = torch.stack(imagined_deters, dim=0)\n",
    "    values = torch.stack(values, dim=0)\n",
    "    rewards = torch.stack(rewards, dim=0)\n",
    "    \n",
    "    returns = lambda_return(rewards, values,0.99, 0.95)\n",
    "    \n",
    "    critic_loss = nn.functional.mse_loss(values[1:],returns[1:].detach())\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    torch.nn.utils.clip_grad_norm_(value.parameters(), max_norm=100)\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    actor_loss = -returns.mean()\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(actor.parameters(), max_norm=100)\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    print(\"actor loss: \",actor_loss.item(),\", critic loss: \",critic_loss.item(),sep='')\n",
    "    \n",
    "    return total_kl_loss/(seq_len-1), total_reconstruction_loss/(seq_len-1), total_reward_loss/(seq_len-1), actor_loss.item(), critic_loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim=64\n",
    "deterministic_dim=256\n",
    "model_lr=1e-4\n",
    "actor_critc_lr=1e-4\n",
    "transition_representation=TransitionRepresentationModel(state_dim, action_dim).to(device)\n",
    "observation=ObservationModel(state_dim,deterministic_dim, obs_shape[2]).to(device)\n",
    "reward=RewardModel(state_dim,deterministic_dim).to(device)\n",
    "\n",
    "agent=Agent(state_dim,deterministic_dim, action_dim).to(device)\n",
    "value=ValueModel(state_dim,deterministic_dim).to(device)\n",
    "\n",
    "model_params = list(transition_representation.parameters()) + list(observation.parameters()) + list(reward.parameters())\n",
    "model_optimizer = optim.Adam(model_params, lr=model_lr)\n",
    "actor_optimizer = optim.Adam(agent.parameters(), lr=actor_critc_lr)\n",
    "critic_optimizer = optim.Adam(value.parameters(), lr=actor_critc_lr)\n",
    "\n",
    "#state, action, reward, next_state, done 저장하고 sampling 가능\n",
    "replay_buffer = ReplayBufferSeq(100000)\n",
    "logger = Logger('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting seed data...\n",
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:52<00:00, 10.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:32<00:00, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-04 02:35:53,613 global_step: 0,train_score: -50.311584079869704, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9013/1336153294.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  obs_seq = torch.tensor(obs_seq, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_9013/1336153294.py:55: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  reward_loss = nn.functional.mse_loss(reward_pred, reward)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training actor, critic...\n",
      "actor loss: -0.3359428346157074, critic loss: 0.13944114744663239\n",
      "2024-10-04 02:36:10,236 global_step: 0,epoch: 0, kl_loss: 0.04735853876063258, reconst_loss: 0.0641889999411544, reward_loss: 0.25450107428644386, actor_loss: -0.3359428346157074, critic_loss: 0.13944114744663239, \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 64\n",
    "seq_len = 50\n",
    "\n",
    "world_episodes = 3\n",
    "update_step = 20\n",
    "\n",
    "seed_episodes = 5\n",
    "test_interval = 3\n",
    "save_interval = 20\n",
    "print(\"collecting seed data...\")\n",
    "collect_data(env,state_dim, transition_representation, agent, replay_buffer, seed_episodes, device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_score=collect_data(env,state_dim, transition_representation, agent, replay_buffer, world_episodes, device)\n",
    "    logger.log(epoch*update_step,train_score=train_score)\n",
    "\n",
    "    if len(replay_buffer) < batch_size*seq_len:\n",
    "        continue\n",
    "    print(len(replay_buffer))\n",
    "    \n",
    "    #train world model and actor, critic\n",
    "    for _ in range(update_step):\n",
    "        batch = replay_buffer.sample_seq(batch_size, seq_len)\n",
    "        kl_loss,reconst_loss, reward_loss, actor_loss, critic_loss=train(batch,state_dim,deterministic_dim, device, transition_representation, reward, observation, agent, value, model_optimizer, actor_optimizer, critic_optimizer)\n",
    "        logger.log(epoch*update_step+_,epoch=epoch, kl_loss=kl_loss, reconst_loss=reconst_loss, reward_loss=reward_loss, actor_loss=actor_loss, critic_loss=critic_loss)\n",
    "\n",
    "    if epoch % test_interval == 0:\n",
    "        test_score=collect_data(env,state_dim, transition_representation, agent, replay_buffer, world_episodes, device,training=False)\n",
    "        logger.log(epoch*update_step,test_score=test_score)\n",
    "    if epoch % save_interval == 0:\n",
    "        torch.save(transition_representation.state_dict(), 'transition_representation.pth')\n",
    "        torch.save(observation.state_dict(), 'observation.pth')\n",
    "        torch.save(reward.state_dict(), 'reward.pth')\n",
    "        torch.save(agent.state_dict(), 'agent.pth')\n",
    "        torch.save(value.state_dict(), 'value.pth')\n",
    "torch.save(transition_representation.state_dict(), 'transition_representation.pth')\n",
    "torch.save(observation.state_dict(), 'observation.pth')\n",
    "torch.save(reward.state_dict(), 'reward.pth')\n",
    "torch.save(agent.state_dict(), 'agent.pth')\n",
    "torch.save(value.state_dict(), 'value.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
