{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mingu/OneDrive/바탕 화면/성균관대/리서치인턴/공부/RL_study/dreamer\n"
     ]
    }
   ],
   "source": [
    "%cd RL_study/dreamer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 01:02:27.405868: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-04 01:02:27.413680: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-04 01:02:27.420612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-04 01:02:27.432515: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-04 01:02:27.436037: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-04 01:02:27.444078: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-04 01:02:28.110985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "\n",
    "from tqdm import tqdm\n",
    "from models import *\n",
    "from logger import Logger\n",
    "\n",
    "\n",
    "env = gym.make('CarRacing-v2')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space: 3, obs shape: (96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "action_dim = env.action_space.shape[0]\n",
    "obs_shape = env.observation_space.shape\n",
    "print(\"action space: \",action_dim,\", obs shape: \", obs_shape,sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env,state_dim, transition_representation, agent,replay_buffer, num_episode, device, training=True):\n",
    "    print(\"collecting data...\")\n",
    "    score=0\n",
    "    for _ in tqdm(range(num_episode)):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        experience = []\n",
    "        prev_state = torch.zeros(1, state_dim).to(device)\n",
    "        prev_deter = transition_representation.init_hidden(1).to(device)\n",
    "        prev_action = torch.zeros(1, action_dim).to(device)\n",
    "        with torch.no_grad():\n",
    "            while not done:\n",
    "                #obs(96x96x3) -> (3x96x96) -> (1x3x96x96)\n",
    "                obs = torch.tensor(obs, dtype=torch.float32).permute(2,0,1).unsqueeze(0).to(device)/255\n",
    "                # s_t-1, a_t-1, o_t-1 -> s_t\n",
    "                posterior_mean, posterior_std, prev_deter = transition_representation.posterior(prev_state, prev_action, prev_deter,obs)\n",
    "                cur_state = posterior_mean + posterior_std*torch.normal(0, 1, posterior_mean.size()).to(device)\n",
    "\n",
    "                action_mu, action_std = agent(cur_state, prev_deter)\n",
    "                eps = torch.normal(0, 1, (1,action_dim)).to(device)\n",
    "                if training:\n",
    "                    cur_action = torch.tanh(action_mu + action_std*eps)\n",
    "                else:\n",
    "                    cur_action = torch.tanh(action_mu)\n",
    "                next_obs, reward, terminated, truncated, info  = env.step(cur_action[0].cpu().numpy())\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                experience.append((np.array(obs.squeeze(0).cpu()), np.array(cur_action.squeeze(0).detach().cpu()), reward, done))\n",
    "                \n",
    "                obs = next_obs\n",
    "                prev_state = cur_state\n",
    "                prev_action = cur_action\n",
    "                score+=reward\n",
    "        if training:\n",
    "            for exp in experience:\n",
    "                replay_buffer.push(exp)\n",
    "    return score/num_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_return(rewards, values, gamma, lambda_):\n",
    "    # rewards, values : (Horizon+1, seq*batch)\n",
    "    # 어렵다\n",
    "    V_lambda = torch.zeros_like(rewards, device=rewards.device)\n",
    "\n",
    "    H = rewards.shape[0] - 1\n",
    "    V_n = torch.zeros_like(rewards, device=rewards.device)\n",
    "    V_n[H] = values[H]\n",
    "    for n in range(1, H+1):\n",
    "        # n-step 계산 하기 위함\n",
    "        # 각 step의 value 목표\n",
    "        V_n[:-n] = (gamma ** n) * values[n:]\n",
    "        for k in range(1, n+1):\n",
    "            # n step의 reward 합 진행\n",
    "            if k == n:\n",
    "                V_n[:-n] += (gamma ** (n-1)) * rewards[k:]\n",
    "            else:\n",
    "                V_n[:-n] += (gamma ** (k-1)) * rewards[k:-n+k]\n",
    "\n",
    "        # add lambda_ weighted n-step target to compute lambda target\n",
    "        if n == H:\n",
    "            V_lambda += (lambda_ ** (H-1)) * V_n\n",
    "        else:\n",
    "            V_lambda += (1 - lambda_) * (lambda_ ** (n-1)) * V_n\n",
    "            \n",
    "    return V_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch,state_dim,deterministic_dim, device, transition_representation, reward_model, observation, actor, value, model_optimizer, actor_optimizer, critic_optimizer):\n",
    "    obs_seq = []\n",
    "    action_seq = []\n",
    "    reward_seq = []\n",
    "    #batch = batch, seq, (obs, action, reward, done)\n",
    "    for seq in batch:\n",
    "        obs_temp=[]\n",
    "        action_temp=[]\n",
    "        reward_temp=[]\n",
    "        for (obs, action, reward, done) in seq:\n",
    "            obs_temp.append(obs)\n",
    "            action_temp.append(action)\n",
    "            reward_temp.append(reward)\n",
    "        obs_seq.append(obs_temp)\n",
    "        action_seq.append(action_temp)\n",
    "        reward_seq.append(reward_temp)\n",
    "    obs_seq = torch.tensor(obs_seq, dtype=torch.float32).to(device)\n",
    "    action_seq = torch.tensor(action_seq, dtype=torch.float32).to(device)\n",
    "    reward_seq = torch.tensor(reward_seq, dtype=torch.float32).to(device)\n",
    "    batch_size, seq_len, _, _, _ = obs_seq.size()\n",
    "    \n",
    "    prev_deter = transition_representation.init_hidden(batch_size).to(device)\n",
    "    prev_state = torch.zeros(batch_size, state_dim).to(device)\n",
    "    \n",
    "    states = torch.zeros(seq_len,batch_size, state_dim).to(device)\n",
    "    deters = torch.zeros(seq_len,batch_size, deterministic_dim).to(device)\n",
    "    \n",
    "    beta=0.1 #kl조절\n",
    "    imagine_horizon=15\n",
    "    gamma=0.99\n",
    "    lambda_=0.95\n",
    "    kl_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    reward_loss = 0\n",
    "    \n",
    "    total_kl_loss = 0\n",
    "    total_reconstruction_loss = 0\n",
    "    total_reward_loss = 0\n",
    "    \n",
    "    action_prev = action_seq[:, 0].to(device)\n",
    "    total_loss=torch.zeros(1).to(device)\n",
    "    for t in range(1,seq_len):\n",
    "        obs = obs_seq[:, t].to(device)\n",
    "        action = action_seq[:, t].to(device)\n",
    "        reward = reward_seq[:, t].to(device)\n",
    "        prior_mean, prior_std, _ = transition_representation(prev_state, action_prev, prev_deter)\n",
    "        posterior_mean, posterior_std, cur_deter = transition_representation.posterior(prev_state, action_prev, prev_deter,obs)\n",
    "        \n",
    "        state = posterior_mean + posterior_std*torch.normal(0, 1, posterior_mean.size()).to(device)\n",
    "        obs_pred = observation(state, cur_deter)\n",
    "        reconstruction_loss = nn.functional.mse_loss(obs_pred, obs)\n",
    "        \n",
    "        \n",
    "        reward_pred = reward_model(state, cur_deter)\n",
    "        reward_loss = nn.functional.mse_loss(reward_pred, reward)\n",
    "        \n",
    "        prior = Normal(prior_mean, prior_std)\n",
    "        posterior = Normal(posterior_mean, posterior_std)\n",
    "        kl_loss = kl_divergence(posterior, prior).mean()\n",
    "\n",
    "        \n",
    "        total_loss += reconstruction_loss + reward_loss + beta*kl_loss\n",
    "\n",
    "        action_prev = action\n",
    "        prev_state = state\n",
    "        prev_deter = cur_deter\n",
    "        \n",
    "        states[t] = state\n",
    "        deters[t] = cur_deter\n",
    "        \n",
    "        total_kl_loss += kl_loss.item()\n",
    "        total_reconstruction_loss += reconstruction_loss.item()\n",
    "        total_reward_loss += reward_loss.item()\n",
    "    model_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    model_optimizer.step()\n",
    "\n",
    "    \n",
    "    ##actor, critic 학습\n",
    "    \n",
    "    #states (seq, batch, state_dim) -> (seq*batch, state_dim)\n",
    "    #deters (seq, batch, deterministic_dim) -> (seq*batch, deterministic_dim)\n",
    "    states = states.view(-1, state_dim).detach()\n",
    "    deters = deters.view(-1, deterministic_dim).detach()\n",
    "    \n",
    "    imagined_states = [states]\n",
    "    imagined_deters = [deters]\n",
    "    \n",
    "    rewards = []\n",
    "    values = []\n",
    "    \n",
    "    \n",
    "    rewards.append(reward_model(states, deters).squeeze())\n",
    "    values.append(value(states, deters).squeeze())\n",
    "    \n",
    "    for t in range(1,imagine_horizon+1):\n",
    "        action_mu, action_std = actor(imagined_states[t-1], imagined_deters[t-1])\n",
    "        eps = torch.normal(0, 1, (action_mu.size())).to(device)\n",
    "        action = torch.tanh(action_mu + action_std*eps)\n",
    "        \n",
    "        prior_mean, prior_std, deter = transition_representation(imagined_states[t-1], action, imagined_deters[t-1])\n",
    "        state = prior_mean + prior_std*torch.normal(0, 1, prior_mean.size()).to(device)\n",
    "        \n",
    "        imagined_states.append(state)\n",
    "        imagined_deters.append(deter)\n",
    "        \n",
    "        rewards.append(reward_model(imagined_states[t], imagined_deters[t]).squeeze())\n",
    "        values.append( value(imagined_states[t], imagined_deters[t]).squeeze())\n",
    "    \n",
    "    imagined_states = torch.stack(imagined_states, dim=0)\n",
    "    imagined_deters = torch.stack(imagined_deters, dim=0)\n",
    "    values = torch.stack(values, dim=0)\n",
    "    rewards = torch.stack(rewards, dim=0)\n",
    "    \n",
    "    returns = lambda_return(rewards, values,0.99, 0.95)\n",
    "    \n",
    "    critic_loss = nn.functional.mse_loss(values[1:],returns[1:].detach())\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    torch.nn.utils.clip_grad_norm_(value.parameters(), max_norm=100)\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    actor_loss = -returns.mean()\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(actor.parameters(), max_norm=100)\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    print(\"actor loss: \",actor_loss.item(),\", critic loss: \",critic_loss.item(),sep='')\n",
    "    \n",
    "    return total_kl_loss/(seq_len-1), total_reconstruction_loss/(seq_len-1), total_reward_loss/(seq_len-1), actor_loss.item(), critic_loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim=64\n",
    "deterministic_dim=256\n",
    "model_lr=1e-4\n",
    "actor_critc_lr=1e-4\n",
    "transition_representation=TransitionRepresentationModel(state_dim, action_dim).to(device)\n",
    "observation=ObservationModel(state_dim,deterministic_dim, obs_shape[2]).to(device)\n",
    "reward=RewardModel(state_dim,deterministic_dim).to(device)\n",
    "\n",
    "agent=Agent(state_dim,deterministic_dim, action_dim).to(device)\n",
    "value=ValueModel(state_dim,deterministic_dim).to(device)\n",
    "\n",
    "model_params = list(transition_representation.parameters()) + list(observation.parameters()) + list(reward.parameters())\n",
    "model_optimizer = optim.Adam(model_params, lr=model_lr)\n",
    "actor_optimizer = optim.Adam(agent.parameters(), lr=actor_critc_lr)\n",
    "critic_optimizer = optim.Adam(value.parameters(), lr=actor_critc_lr)\n",
    "\n",
    "#state, action, reward, next_state, done 저장하고 sampling 가능\n",
    "replay_buffer = ReplayBufferSeq(100000)\n",
    "logger = Logger('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting seed data...\n",
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:57<00:00, 11.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-04 01:03:38,565 global_step: 0,train_score: -51.127819548872935, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_7049/1976329320.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  obs_seq = torch.tensor(obs_seq, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_7049/1976329320.py:55: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  reward_loss = nn.functional.mse_loss(reward_pred, reward)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor loss: -0.16721706092357635, critic loss: 0.02095862850546837\n",
      "2024-10-04 01:03:54,752 global_step: 0,epoch: 0, kl_loss: 0.04724697250758811, reconst_loss: 0.05853355789975244, reward_loss: 0.21097925012665136, actor_loss: -0.16721706092357635, critic_loss: 0.02095862850546837, \n",
      "actor loss: -0.08586236834526062, critic loss: 0.0065489462576806545\n",
      "2024-10-04 01:04:08,117 global_step: 1,epoch: 0, kl_loss: 0.04654910202062099, reconst_loss: 0.0583842969974693, reward_loss: 0.18402682248578997, actor_loss: -0.08586236834526062, critic_loss: 0.0065489462576806545, \n",
      "actor loss: -0.007309821899980307, critic loss: 0.013196372427046299\n",
      "2024-10-04 01:04:19,666 global_step: 2,epoch: 0, kl_loss: 0.0455610510532041, reconst_loss: 0.0580580295348654, reward_loss: 0.15516277976638201, actor_loss: -0.007309821899980307, critic_loss: 0.013196372427046299, \n",
      "actor loss: 0.08251304179430008, critic loss: 0.039441704750061035\n",
      "2024-10-04 01:04:30,616 global_step: 3,epoch: 0, kl_loss: 0.044854602011452825, reconst_loss: 0.05798936551626848, reward_loss: 0.12953136921195046, actor_loss: 0.08251304179430008, critic_loss: 0.039441704750061035, \n",
      "actor loss: 0.17380709946155548, critic loss: 0.0810459777712822\n",
      "2024-10-04 01:04:41,791 global_step: 4,epoch: 0, kl_loss: 0.04433408207070006, reconst_loss: 0.05800881084738946, reward_loss: 0.18000492244977884, actor_loss: 0.17380709946155548, critic_loss: 0.0810459777712822, \n",
      "actor loss: 0.26262110471725464, critic loss: 0.13402028381824493\n",
      "2024-10-04 01:04:53,114 global_step: 5,epoch: 0, kl_loss: 0.043167102796842854, reconst_loss: 0.05740542937906421, reward_loss: 0.11552208873006153, actor_loss: 0.26262110471725464, critic_loss: 0.13402028381824493, \n",
      "actor loss: 0.34081733226776123, critic loss: 0.19003210961818695\n",
      "2024-10-04 01:05:04,198 global_step: 6,epoch: 0, kl_loss: 0.04282761511525938, reconst_loss: 0.05788769185238955, reward_loss: 0.17958256148742702, actor_loss: 0.34081733226776123, critic_loss: 0.19003210961818695, \n",
      "actor loss: 0.4052536189556122, critic loss: 0.23943553864955902\n",
      "2024-10-04 01:05:15,147 global_step: 7,epoch: 0, kl_loss: 0.04196697798063408, reconst_loss: 0.05790207618657423, reward_loss: 0.1778285173884574, actor_loss: 0.4052536189556122, critic_loss: 0.23943553864955902, \n",
      "actor loss: 0.4538441300392151, critic loss: 0.2759075462818146\n",
      "2024-10-04 01:05:26,153 global_step: 8,epoch: 0, kl_loss: 0.04134222462165112, reconst_loss: 0.05796864461533877, reward_loss: 0.1972704630425884, actor_loss: 0.4538441300392151, critic_loss: 0.2759075462818146, \n",
      "actor loss: 0.4882766604423523, critic loss: 0.29844409227371216\n",
      "2024-10-04 01:05:36,977 global_step: 9,epoch: 0, kl_loss: 0.0405166148901822, reconst_loss: 0.05725106262430853, reward_loss: 0.1390571405265328, actor_loss: 0.4882766604423523, critic_loss: 0.29844409227371216, \n",
      "actor loss: 0.5080228447914124, critic loss: 0.30222228169441223\n",
      "2024-10-04 01:05:47,949 global_step: 10,epoch: 0, kl_loss: 0.03987616000278872, reconst_loss: 0.05750865466436561, reward_loss: 0.16896629641046368, actor_loss: 0.5080228447914124, critic_loss: 0.30222228169441223, \n",
      "actor loss: 0.5200671553611755, critic loss: 0.2966991066932678\n",
      "2024-10-04 01:05:58,766 global_step: 11,epoch: 0, kl_loss: 0.03874212743866504, reconst_loss: 0.05739783624909362, reward_loss: 0.17472360603516085, actor_loss: 0.5200671553611755, critic_loss: 0.2966991066932678, \n",
      "actor loss: 0.5149438381195068, critic loss: 0.2719060480594635\n",
      "2024-10-04 01:06:09,553 global_step: 12,epoch: 0, kl_loss: 0.03806889868740525, reconst_loss: 0.05804913141289536, reward_loss: 0.2719287358222016, actor_loss: 0.5149438381195068, critic_loss: 0.2719060480594635, \n",
      "actor loss: 0.5048742294311523, critic loss: 0.24319900572299957\n",
      "2024-10-04 01:06:20,353 global_step: 13,epoch: 0, kl_loss: 0.03750139234435498, reconst_loss: 0.05777315156800406, reward_loss: 0.22701983484653376, actor_loss: 0.5048742294311523, critic_loss: 0.24319900572299957, \n",
      "actor loss: 0.4891592562198639, critic loss: 0.2092124968767166\n",
      "2024-10-04 01:06:31,249 global_step: 14,epoch: 0, kl_loss: 0.03650044223616774, reconst_loss: 0.05677839847547667, reward_loss: 0.19942794617845164, actor_loss: 0.4891592562198639, critic_loss: 0.2092124968767166, \n",
      "actor loss: 0.471139520406723, critic loss: 0.17660707235336304\n",
      "2024-10-04 01:06:42,137 global_step: 15,epoch: 0, kl_loss: 0.03584433805520589, reconst_loss: 0.05712888260581055, reward_loss: 0.21233258773905359, actor_loss: 0.471139520406723, critic_loss: 0.17660707235336304, \n",
      "actor loss: 0.453083336353302, critic loss: 0.14609280228614807\n",
      "2024-10-04 01:06:53,251 global_step: 16,epoch: 0, kl_loss: 0.035308473011744876, reconst_loss: 0.05645381286740303, reward_loss: 0.17172858721044446, actor_loss: 0.453083336353302, critic_loss: 0.14609280228614807, \n",
      "actor loss: 0.4380660951137543, critic loss: 0.12111842632293701\n",
      "2024-10-04 01:07:04,529 global_step: 17,epoch: 0, kl_loss: 0.03454275250586928, reconst_loss: 0.055941079039962927, reward_loss: 0.15563903110605493, actor_loss: 0.4380660951137543, critic_loss: 0.12111842632293701, \n",
      "actor loss: 0.4240230321884155, critic loss: 0.09933245927095413\n",
      "2024-10-04 01:07:15,718 global_step: 18,epoch: 0, kl_loss: 0.033886536200321754, reconst_loss: 0.0563343773995127, reward_loss: 0.20356648481850112, actor_loss: 0.4240230321884155, critic_loss: 0.09933245927095413, \n",
      "actor loss: 0.41498908400535583, critic loss: 0.08274084329605103\n",
      "2024-10-04 01:07:26,730 global_step: 19,epoch: 0, kl_loss: 0.03340645548792518, reconst_loss: 0.05601057501471773, reward_loss: 0.1805233255893524, actor_loss: 0.41498908400535583, critic_loss: 0.08274084329605103, \n",
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-04 01:07:37,593 global_step: 0,test_score: -92.53731343283484, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:12<00:00, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-04 01:07:50,240 global_step: 20,train_score: -63.503649635037185, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor loss: 0.40864279866218567, critic loss: 0.06950075179338455\n",
      "2024-10-04 01:08:01,156 global_step: 20,epoch: 1, kl_loss: 0.03281098168001187, reconst_loss: 0.0550857380184592, reward_loss: 0.1697316064252233, actor_loss: 0.40864279866218567, critic_loss: 0.06950075179338455, \n",
      "actor loss: 0.4085176885128021, critic loss: 0.060808081179857254\n",
      "2024-10-04 01:08:12,098 global_step: 21,epoch: 1, kl_loss: 0.03223165118952795, reconst_loss: 0.05479680815217446, reward_loss: 0.18455084345341488, actor_loss: 0.4085176885128021, critic_loss: 0.060808081179857254, \n",
      "actor loss: 0.4122876524925232, critic loss: 0.054538071155548096\n",
      "2024-10-04 01:08:23,029 global_step: 22,epoch: 1, kl_loss: 0.03185971272748192, reconst_loss: 0.054573401048475384, reward_loss: 0.1842106553982487, actor_loss: 0.4122876524925232, critic_loss: 0.054538071155548096, \n",
      "actor loss: 0.42517340183258057, critic loss: 0.052988890558481216\n",
      "2024-10-04 01:08:34,166 global_step: 23,epoch: 1, kl_loss: 0.03114822572477314, reconst_loss: 0.05376543728064517, reward_loss: 0.1274109357226716, actor_loss: 0.42517340183258057, critic_loss: 0.052988890558481216, \n",
      "actor loss: 0.4401167333126068, critic loss: 0.05279914289712906\n",
      "2024-10-04 01:08:45,432 global_step: 24,epoch: 1, kl_loss: 0.030250737053931366, reconst_loss: 0.05336383692159945, reward_loss: 0.1844861658375558, actor_loss: 0.4401167333126068, critic_loss: 0.05279914289712906, \n",
      "actor loss: 0.4608698785305023, critic loss: 0.05508632957935333\n",
      "2024-10-04 01:08:56,382 global_step: 25,epoch: 1, kl_loss: 0.02993941927632811, reconst_loss: 0.05298379785856422, reward_loss: 0.17674590222898642, actor_loss: 0.4608698785305023, critic_loss: 0.05508632957935333, \n",
      "actor loss: 0.48465627431869507, critic loss: 0.05919243395328522\n",
      "2024-10-04 01:09:07,446 global_step: 26,epoch: 1, kl_loss: 0.02932656915592296, reconst_loss: 0.05220317027094413, reward_loss: 0.16570690690007592, actor_loss: 0.48465627431869507, critic_loss: 0.05919243395328522, \n",
      "actor loss: 0.5135375261306763, critic loss: 0.06610885262489319\n",
      "2024-10-04 01:09:18,476 global_step: 27,epoch: 1, kl_loss: 0.02879364965768645, reconst_loss: 0.051508789539945365, reward_loss: 0.13795537525806956, actor_loss: 0.5135375261306763, critic_loss: 0.06610885262489319, \n",
      "actor loss: 0.5458716750144958, critic loss: 0.07503274083137512\n",
      "2024-10-04 01:09:29,346 global_step: 28,epoch: 1, kl_loss: 0.02824994094897898, reconst_loss: 0.050787011731644065, reward_loss: 0.14368407650166476, actor_loss: 0.5458716750144958, critic_loss: 0.07503274083137512, \n",
      "actor loss: 0.57796311378479, critic loss: 0.08417411148548126\n",
      "2024-10-04 01:09:40,250 global_step: 29,epoch: 1, kl_loss: 0.027774513941448257, reconst_loss: 0.05065956544511172, reward_loss: 0.19048797264600134, actor_loss: 0.57796311378479, critic_loss: 0.08417411148548126, \n",
      "actor loss: 0.6123064160346985, critic loss: 0.09521258622407913\n",
      "2024-10-04 01:09:51,208 global_step: 30,epoch: 1, kl_loss: 0.027356682036414136, reconst_loss: 0.05020364959325109, reward_loss: 0.17298966401009536, actor_loss: 0.6123064160346985, critic_loss: 0.09521258622407913, \n",
      "actor loss: 0.6453051567077637, critic loss: 0.10566073656082153\n",
      "2024-10-04 01:10:02,054 global_step: 31,epoch: 1, kl_loss: 0.026971030495680715, reconst_loss: 0.04944648289558839, reward_loss: 0.17498579209822476, actor_loss: 0.6453051567077637, critic_loss: 0.10566073656082153, \n",
      "actor loss: 0.6790907382965088, critic loss: 0.11701563745737076\n",
      "2024-10-04 01:10:13,067 global_step: 32,epoch: 1, kl_loss: 0.02623919213228688, reconst_loss: 0.04948879451471932, reward_loss: 0.1786309106665074, actor_loss: 0.6790907382965088, critic_loss: 0.11701563745737076, \n",
      "actor loss: 0.7161683440208435, critic loss: 0.1300080567598343\n",
      "2024-10-04 01:10:23,988 global_step: 33,epoch: 1, kl_loss: 0.02598319832729746, reconst_loss: 0.048416955814677844, reward_loss: 0.15594966140129052, actor_loss: 0.7161683440208435, critic_loss: 0.1300080567598343, \n",
      "actor loss: 0.7427323460578918, critic loss: 0.13608936965465546\n",
      "2024-10-04 01:10:34,968 global_step: 34,epoch: 1, kl_loss: 0.02553132742795409, reconst_loss: 0.04885483677594029, reward_loss: 0.25236630176754704, actor_loss: 0.7427323460578918, critic_loss: 0.13608936965465546, \n",
      "actor loss: 0.7706806659698486, critic loss: 0.14155800640583038\n",
      "2024-10-04 01:10:45,880 global_step: 35,epoch: 1, kl_loss: 0.025042487453308185, reconst_loss: 0.04895864852837154, reward_loss: 0.21951954082671402, actor_loss: 0.7706806659698486, critic_loss: 0.14155800640583038, \n",
      "actor loss: 0.7970470786094666, critic loss: 0.14619474112987518\n",
      "2024-10-04 01:10:57,130 global_step: 36,epoch: 1, kl_loss: 0.02452864052908381, reconst_loss: 0.047969549818306555, reward_loss: 0.1558528176736923, actor_loss: 0.7970470786094666, critic_loss: 0.14619474112987518, \n",
      "actor loss: 0.8201107382774353, critic loss: 0.14719434082508087\n",
      "2024-10-04 01:11:08,019 global_step: 37,epoch: 1, kl_loss: 0.024135874273084407, reconst_loss: 0.048288303674483786, reward_loss: 0.1678029349458651, actor_loss: 0.8201107382774353, critic_loss: 0.14719434082508087, \n",
      "actor loss: 0.8451007604598999, critic loss: 0.14910247921943665\n",
      "2024-10-04 01:11:18,843 global_step: 38,epoch: 1, kl_loss: 0.02363803008172129, reconst_loss: 0.04757012723355877, reward_loss: 0.1527883336233089, actor_loss: 0.8451007604598999, critic_loss: 0.14910247921943665, \n",
      "actor loss: 0.8684790134429932, critic loss: 0.14851409196853638\n",
      "2024-10-04 01:11:29,829 global_step: 39,epoch: 1, kl_loss: 0.023356001657832946, reconst_loss: 0.04805735977632659, reward_loss: 0.17868448747797128, actor_loss: 0.8684790134429932, critic_loss: 0.14851409196853638, \n",
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-04 01:11:40,378 global_step: 40,train_score: -73.50993377483466, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor loss: 0.8868411183357239, critic loss: 0.14424102008342743\n",
      "2024-10-04 01:11:51,530 global_step: 40,epoch: 2, kl_loss: 0.02296482807985146, reconst_loss: 0.04810088279904151, reward_loss: 0.18756635884079625, actor_loss: 0.8868411183357239, critic_loss: 0.14424102008342743, \n",
      "actor loss: 0.9032642841339111, critic loss: 0.1380825787782669\n",
      "2024-10-04 01:12:02,518 global_step: 41,epoch: 2, kl_loss: 0.022433251570149953, reconst_loss: 0.04802800274016906, reward_loss: 0.20131853398420296, actor_loss: 0.9032642841339111, critic_loss: 0.1380825787782669, \n",
      "actor loss: 0.9197560548782349, critic loss: 0.13106907904148102\n",
      "2024-10-04 01:12:13,482 global_step: 42,epoch: 2, kl_loss: 0.02230836658225376, reconst_loss: 0.04806892429383434, reward_loss: 0.2067033797645067, actor_loss: 0.9197560548782349, critic_loss: 0.13106907904148102, \n",
      "actor loss: 0.9356011748313904, critic loss: 0.12287493795156479\n",
      "2024-10-04 01:12:24,587 global_step: 43,epoch: 2, kl_loss: 0.02175178408280623, reconst_loss: 0.04718474853707819, reward_loss: 0.14050168421699152, actor_loss: 0.9356011748313904, critic_loss: 0.12287493795156479, \n",
      "actor loss: 0.9532493352890015, critic loss: 0.11684431880712509\n",
      "2024-10-04 01:12:35,427 global_step: 44,epoch: 2, kl_loss: 0.0216495056294513, reconst_loss: 0.04725253293100668, reward_loss: 0.15716141410770693, actor_loss: 0.9532493352890015, critic_loss: 0.11684431880712509, \n",
      "actor loss: 0.9784161448478699, critic loss: 0.11480732262134552\n",
      "2024-10-04 01:12:46,444 global_step: 45,epoch: 2, kl_loss: 0.021084367649211566, reconst_loss: 0.046554055977232604, reward_loss: 0.14112152758871718, actor_loss: 0.9784161448478699, critic_loss: 0.11480732262134552, \n",
      "actor loss: 0.9999060034751892, critic loss: 0.11061883717775345\n",
      "2024-10-04 01:12:57,221 global_step: 46,epoch: 2, kl_loss: 0.020650123531112865, reconst_loss: 0.04696896793890973, reward_loss: 0.1551211137628677, actor_loss: 0.9999060034751892, critic_loss: 0.11061883717775345, \n",
      "actor loss: 1.0222886800765991, critic loss: 0.10677624493837357\n",
      "2024-10-04 01:13:08,116 global_step: 47,epoch: 2, kl_loss: 0.020252248500378763, reconst_loss: 0.04673456157348594, reward_loss: 0.18646848074407602, actor_loss: 1.0222886800765991, critic_loss: 0.10677624493837357, \n",
      "actor loss: 1.0484936237335205, critic loss: 0.10452421009540558\n",
      "2024-10-04 01:13:18,876 global_step: 48,epoch: 2, kl_loss: 0.019983794487898752, reconst_loss: 0.04612798966011222, reward_loss: 0.17939051042063808, actor_loss: 1.0484936237335205, critic_loss: 0.10452421009540558, \n",
      "actor loss: 1.0761497020721436, critic loss: 0.10279643535614014\n",
      "2024-10-04 01:13:29,864 global_step: 49,epoch: 2, kl_loss: 0.019700993607961098, reconst_loss: 0.04616948414822014, reward_loss: 0.14575485733090615, actor_loss: 1.0761497020721436, critic_loss: 0.10279643535614014, \n",
      "actor loss: 1.106972336769104, critic loss: 0.10271777212619781\n",
      "2024-10-04 01:13:40,634 global_step: 50,epoch: 2, kl_loss: 0.019331854867881963, reconst_loss: 0.046529959431108166, reward_loss: 0.17141953436182622, actor_loss: 1.106972336769104, critic_loss: 0.10271777212619781, \n",
      "actor loss: 1.1367484331130981, critic loss: 0.10128561407327652\n",
      "2024-10-04 01:13:51,473 global_step: 51,epoch: 2, kl_loss: 0.018906721486519947, reconst_loss: 0.04599325731396675, reward_loss: 0.16444859637080558, actor_loss: 1.1367484331130981, critic_loss: 0.10128561407327652, \n",
      "actor loss: 1.1664848327636719, critic loss: 0.10008431226015091\n",
      "2024-10-04 01:14:02,232 global_step: 52,epoch: 2, kl_loss: 0.01855645895869072, reconst_loss: 0.04663883225650203, reward_loss: 0.24546854601868864, actor_loss: 1.1664848327636719, critic_loss: 0.10008431226015091, \n",
      "actor loss: 1.1973309516906738, critic loss: 0.09873424470424652\n",
      "2024-10-04 01:14:13,024 global_step: 53,epoch: 2, kl_loss: 0.0183170132804662, reconst_loss: 0.04558277533066516, reward_loss: 0.14418091032207392, actor_loss: 1.1973309516906738, critic_loss: 0.09873424470424652, \n",
      "actor loss: 1.2377334833145142, critic loss: 0.10137633979320526\n",
      "2024-10-04 01:14:23,885 global_step: 54,epoch: 2, kl_loss: 0.017948116653370767, reconst_loss: 0.045716251341664065, reward_loss: 0.16073366267397543, actor_loss: 1.2377334833145142, critic_loss: 0.10137633979320526, \n",
      "actor loss: 1.271934151649475, critic loss: 0.10120243579149246\n",
      "2024-10-04 01:14:34,885 global_step: 55,epoch: 2, kl_loss: 0.017673555657039493, reconst_loss: 0.04601658492976306, reward_loss: 0.15777281658933023, actor_loss: 1.271934151649475, critic_loss: 0.10120243579149246, \n",
      "actor loss: 1.31449556350708, critic loss: 0.10509610176086426\n",
      "2024-10-04 01:14:45,802 global_step: 56,epoch: 2, kl_loss: 0.017335696331206327, reconst_loss: 0.04553837298738713, reward_loss: 0.15575582137787525, actor_loss: 1.31449556350708, critic_loss: 0.10509610176086426, \n",
      "actor loss: 1.3573499917984009, critic loss: 0.10684069991111755\n",
      "2024-10-04 01:14:56,738 global_step: 57,epoch: 2, kl_loss: 0.01709134458345646, reconst_loss: 0.045454742881108304, reward_loss: 0.1637005882518252, actor_loss: 1.3573499917984009, critic_loss: 0.10684069991111755, \n",
      "actor loss: 1.4006301164627075, critic loss: 0.1087249144911766\n",
      "2024-10-04 01:15:07,707 global_step: 58,epoch: 2, kl_loss: 0.01676278716970083, reconst_loss: 0.04568667246066794, reward_loss: 0.23057998669789914, actor_loss: 1.4006301164627075, critic_loss: 0.1087249144911766, \n",
      "actor loss: 1.4470744132995605, critic loss: 0.1106794998049736\n",
      "2024-10-04 01:15:18,722 global_step: 59,epoch: 2, kl_loss: 0.01643393241933414, reconst_loss: 0.04532938526601207, reward_loss: 0.19077681616062717, actor_loss: 1.4470744132995605, critic_loss: 0.1106794998049736, \n",
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-04 01:15:29,178 global_step: 60,train_score: -79.79797979797951, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor loss: 1.4963157176971436, critic loss: 0.11335998773574829\n",
      "2024-10-04 01:15:40,244 global_step: 60,epoch: 3, kl_loss: 0.01630589144532474, reconst_loss: 0.04471880334372423, reward_loss: 0.15153176794113704, actor_loss: 1.4963157176971436, critic_loss: 0.11335998773574829, \n",
      "actor loss: 1.5492439270019531, critic loss: 0.11587461084127426\n",
      "2024-10-04 01:15:51,301 global_step: 61,epoch: 3, kl_loss: 0.016071407982072204, reconst_loss: 0.04408758597410455, reward_loss: 0.1501746677546477, actor_loss: 1.5492439270019531, critic_loss: 0.11587461084127426, \n",
      "actor loss: 1.6075255870819092, critic loss: 0.11851570755243301\n",
      "2024-10-04 01:16:02,335 global_step: 62,epoch: 3, kl_loss: 0.015919057113042444, reconst_loss: 0.04427242453913299, reward_loss: 0.1934288083171776, actor_loss: 1.6075255870819092, critic_loss: 0.11851570755243301, \n",
      "actor loss: 1.666734218597412, critic loss: 0.12089784443378448\n",
      "2024-10-04 01:16:13,439 global_step: 63,epoch: 3, kl_loss: 0.01570609815380707, reconst_loss: 0.044304111174174717, reward_loss: 0.1770634490530938, actor_loss: 1.666734218597412, critic_loss: 0.12089784443378448, \n",
      "actor loss: 1.7385048866271973, critic loss: 0.12756365537643433\n",
      "2024-10-04 01:16:24,469 global_step: 64,epoch: 3, kl_loss: 0.01570572275478317, reconst_loss: 0.043403123714485944, reward_loss: 0.14112672060300424, actor_loss: 1.7385048866271973, critic_loss: 0.12756365537643433, \n",
      "actor loss: 1.8187148571014404, critic loss: 0.13057877123355865\n",
      "2024-10-04 01:16:35,394 global_step: 65,epoch: 3, kl_loss: 0.01558933506852814, reconst_loss: 0.04356931051125332, reward_loss: 0.1537849949126379, actor_loss: 1.8187148571014404, critic_loss: 0.13057877123355865, \n",
      "actor loss: 1.8925288915634155, critic loss: 0.13040955364704132\n",
      "2024-10-04 01:16:46,633 global_step: 66,epoch: 3, kl_loss: 0.015562831115319717, reconst_loss: 0.04369607408131872, reward_loss: 0.1784626356615894, actor_loss: 1.8925288915634155, critic_loss: 0.13040955364704132, \n",
      "actor loss: 1.9807069301605225, critic loss: 0.13341869413852692\n",
      "2024-10-04 01:16:57,791 global_step: 67,epoch: 3, kl_loss: 0.015508264388737023, reconst_loss: 0.0432815893116046, reward_loss: 0.18192710764991232, actor_loss: 1.9807069301605225, critic_loss: 0.13341869413852692, \n",
      "actor loss: 2.073773145675659, critic loss: 0.13416223227977753\n",
      "2024-10-04 01:17:09,044 global_step: 68,epoch: 3, kl_loss: 0.01537326123679475, reconst_loss: 0.043014152393657336, reward_loss: 0.156820531193243, actor_loss: 2.073773145675659, critic_loss: 0.13416223227977753, \n",
      "actor loss: 2.1839451789855957, critic loss: 0.13565199077129364\n",
      "2024-10-04 01:17:19,985 global_step: 69,epoch: 3, kl_loss: 0.015233946355934046, reconst_loss: 0.04298283068501219, reward_loss: 0.16909769126594218, actor_loss: 2.1839451789855957, critic_loss: 0.13565199077129364, \n",
      "actor loss: 2.3032140731811523, critic loss: 0.13710686564445496\n",
      "2024-10-04 01:17:31,223 global_step: 70,epoch: 3, kl_loss: 0.014983291376610192, reconst_loss: 0.042233417380829245, reward_loss: 0.16458827071604604, actor_loss: 2.3032140731811523, critic_loss: 0.13710686564445496, \n",
      "actor loss: 2.4399359226226807, critic loss: 0.14271655678749084\n",
      "2024-10-04 01:17:42,153 global_step: 71,epoch: 3, kl_loss: 0.014792364527832488, reconst_loss: 0.042218664851115674, reward_loss: 0.17339205993006804, actor_loss: 2.4399359226226807, critic_loss: 0.14271655678749084, \n",
      "actor loss: 2.5898396968841553, critic loss: 0.14530634880065918\n",
      "2024-10-04 01:17:53,024 global_step: 72,epoch: 3, kl_loss: 0.014530580348278187, reconst_loss: 0.04224300521368883, reward_loss: 0.1423866564873606, actor_loss: 2.5898396968841553, critic_loss: 0.14530634880065918, \n",
      "actor loss: 2.759430170059204, critic loss: 0.15655110776424408\n",
      "2024-10-04 01:18:03,961 global_step: 73,epoch: 3, kl_loss: 0.014138897937931577, reconst_loss: 0.04195485955902508, reward_loss: 0.16905998763371716, actor_loss: 2.759430170059204, critic_loss: 0.15655110776424408, \n",
      "actor loss: 2.97261905670166, critic loss: 0.16818757355213165\n",
      "2024-10-04 01:18:14,975 global_step: 74,epoch: 3, kl_loss: 0.01380949904096826, reconst_loss: 0.041824065452935745, reward_loss: 0.15292115117974428, actor_loss: 2.97261905670166, critic_loss: 0.16818757355213165, \n",
      "actor loss: 3.203439235687256, critic loss: 0.184740349650383\n",
      "2024-10-04 01:18:25,936 global_step: 75,epoch: 3, kl_loss: 0.013476459855897998, reconst_loss: 0.04104707914651656, reward_loss: 0.18428009054718578, actor_loss: 3.203439235687256, critic_loss: 0.184740349650383, \n",
      "actor loss: 3.4630725383758545, critic loss: 0.20470428466796875\n",
      "2024-10-04 01:18:36,787 global_step: 76,epoch: 3, kl_loss: 0.013134112115949392, reconst_loss: 0.04108506471526866, reward_loss: 0.16541774696622957, actor_loss: 3.4630725383758545, critic_loss: 0.20470428466796875, \n",
      "actor loss: 3.785327672958374, critic loss: 0.22001969814300537\n",
      "2024-10-04 01:18:47,754 global_step: 77,epoch: 3, kl_loss: 0.012771006098625307, reconst_loss: 0.040665801994654596, reward_loss: 0.18339438948362152, actor_loss: 3.785327672958374, critic_loss: 0.22001969814300537, \n",
      "actor loss: 4.1752519607543945, critic loss: 0.2456350475549698\n",
      "2024-10-04 01:18:58,722 global_step: 78,epoch: 3, kl_loss: 0.012292521098172483, reconst_loss: 0.039564862847328186, reward_loss: 0.13069151185823566, actor_loss: 4.1752519607543945, critic_loss: 0.2456350475549698, \n",
      "actor loss: 4.605809211730957, critic loss: 0.26620519161224365\n",
      "2024-10-04 01:19:09,535 global_step: 79,epoch: 3, kl_loss: 0.011822153852150147, reconst_loss: 0.03956237686227779, reward_loss: 0.14914649351005804, actor_loss: 4.605809211730957, critic_loss: 0.26620519161224365, \n",
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-04 01:19:19,604 global_step: 60,test_score: -40.35087719298306, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-04 01:19:30,843 global_step: 80,train_score: -65.98639455782379, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor loss: 5.037748336791992, critic loss: 0.31155452132225037\n",
      "2024-10-04 01:19:41,695 global_step: 80,epoch: 4, kl_loss: 0.011375568731098759, reconst_loss: 0.03985837025910008, reward_loss: 0.16434116211092595, actor_loss: 5.037748336791992, critic_loss: 0.31155452132225037, \n",
      "actor loss: 5.4593658447265625, critic loss: 0.3270821273326874\n",
      "2024-10-04 01:19:52,777 global_step: 81,epoch: 4, kl_loss: 0.010876764590870969, reconst_loss: 0.03993202084485365, reward_loss: 0.20035658496413, actor_loss: 5.4593658447265625, critic_loss: 0.3270821273326874, \n",
      "actor loss: 5.959342002868652, critic loss: 0.3752315938472748\n",
      "2024-10-04 01:20:03,683 global_step: 82,epoch: 4, kl_loss: 0.010370833324078394, reconst_loss: 0.03913581819862735, reward_loss: 0.16958629906804737, actor_loss: 5.959342002868652, critic_loss: 0.3752315938472748, \n",
      "actor loss: 6.516692161560059, critic loss: 0.42621833086013794\n",
      "2024-10-04 01:20:14,957 global_step: 83,epoch: 4, kl_loss: 0.009901970064229503, reconst_loss: 0.0390446668832886, reward_loss: 0.2030068444187886, actor_loss: 6.516692161560059, critic_loss: 0.42621833086013794, \n",
      "actor loss: 7.200765132904053, critic loss: 0.496200829744339\n",
      "2024-10-04 01:20:26,311 global_step: 84,epoch: 4, kl_loss: 0.009602285389389311, reconst_loss: 0.03893945442170513, reward_loss: 0.18673933191434006, actor_loss: 7.200765132904053, critic_loss: 0.496200829744339, \n",
      "actor loss: 7.926296710968018, critic loss: 0.5967581868171692\n",
      "2024-10-04 01:20:37,524 global_step: 85,epoch: 4, kl_loss: 0.009250395712727795, reconst_loss: 0.03840164186394945, reward_loss: 0.20381706712140266, actor_loss: 7.926296710968018, critic_loss: 0.5967581868171692, \n",
      "actor loss: 8.76509952545166, critic loss: 0.6950896978378296\n",
      "2024-10-04 01:20:48,707 global_step: 86,epoch: 4, kl_loss: 0.008889826793907856, reconst_loss: 0.03742429089485383, reward_loss: 0.13118226594785798, actor_loss: 8.76509952545166, critic_loss: 0.6950896978378296, \n",
      "actor loss: 9.576862335205078, critic loss: 0.830361545085907\n",
      "2024-10-04 01:20:59,963 global_step: 87,epoch: 4, kl_loss: 0.008590546930779, reconst_loss: 0.037110629054356595, reward_loss: 0.17270104306968576, actor_loss: 9.576862335205078, critic_loss: 0.830361545085907, \n",
      "actor loss: 10.288947105407715, critic loss: 0.9197630286216736\n",
      "2024-10-04 01:21:11,000 global_step: 88,epoch: 4, kl_loss: 0.00831084460856355, reconst_loss: 0.03648868195560514, reward_loss: 0.15964684969203888, actor_loss: 10.288947105407715, critic_loss: 0.9197630286216736, \n",
      "actor loss: 10.891036033630371, critic loss: 1.0046077966690063\n",
      "2024-10-04 01:21:22,291 global_step: 89,epoch: 4, kl_loss: 0.008057239874057015, reconst_loss: 0.03655749909123596, reward_loss: 0.17560176672984143, actor_loss: 10.891036033630371, critic_loss: 1.0046077966690063, \n",
      "actor loss: 11.334303855895996, critic loss: 1.0388528108596802\n",
      "2024-10-04 01:21:33,336 global_step: 90,epoch: 4, kl_loss: 0.00783832016762118, reconst_loss: 0.03604433335820023, reward_loss: 0.16378494953898218, actor_loss: 11.334303855895996, critic_loss: 1.0388528108596802, \n",
      "actor loss: 11.706025123596191, critic loss: 1.0762526988983154\n",
      "2024-10-04 01:21:44,340 global_step: 91,epoch: 4, kl_loss: 0.007649068102924799, reconst_loss: 0.035203810857266794, reward_loss: 0.10628697522963416, actor_loss: 11.706025123596191, critic_loss: 1.0762526988983154, \n",
      "actor loss: 11.92480182647705, critic loss: 1.1494311094284058\n",
      "2024-10-04 01:21:55,590 global_step: 92,epoch: 4, kl_loss: 0.007541988434612143, reconst_loss: 0.03527604416012764, reward_loss: 0.1929009718468831, actor_loss: 11.92480182647705, critic_loss: 1.1494311094284058, \n",
      "actor loss: 12.013222694396973, critic loss: 1.1330275535583496\n",
      "2024-10-04 01:22:06,710 global_step: 93,epoch: 4, kl_loss: 0.0073017862112242345, reconst_loss: 0.03444131372534499, reward_loss: 0.20424000793421754, actor_loss: 12.013222694396973, critic_loss: 1.1330275535583496, \n",
      "actor loss: 12.038556098937988, critic loss: 1.106890082359314\n",
      "2024-10-04 01:22:18,383 global_step: 94,epoch: 4, kl_loss: 0.007122706526852384, reconst_loss: 0.033167235173132956, reward_loss: 0.16028793111481532, actor_loss: 12.038556098937988, critic_loss: 1.106890082359314, \n",
      "actor loss: 11.98537826538086, critic loss: 1.0414339303970337\n",
      "2024-10-04 01:22:29,844 global_step: 95,epoch: 4, kl_loss: 0.006997441673385245, reconst_loss: 0.032270115819208474, reward_loss: 0.1824766724009295, actor_loss: 11.98537826538086, critic_loss: 1.0414339303970337, \n",
      "actor loss: 11.894758224487305, critic loss: 1.0153522491455078\n",
      "2024-10-04 01:22:40,919 global_step: 96,epoch: 4, kl_loss: 0.006928092353426072, reconst_loss: 0.0316200634578661, reward_loss: 0.1697331829642763, actor_loss: 11.894758224487305, critic_loss: 1.0153522491455078, \n",
      "actor loss: 11.73399543762207, critic loss: 0.918404757976532\n",
      "2024-10-04 01:22:52,032 global_step: 97,epoch: 4, kl_loss: 0.006751285485771238, reconst_loss: 0.03101510607770511, reward_loss: 0.1820276767058222, actor_loss: 11.73399543762207, critic_loss: 0.918404757976532, \n",
      "actor loss: 11.512457847595215, critic loss: 0.8711782693862915\n",
      "2024-10-04 01:23:03,216 global_step: 98,epoch: 4, kl_loss: 0.006704980726068725, reconst_loss: 0.030198967540446594, reward_loss: 0.16944547019642303, actor_loss: 11.512457847595215, critic_loss: 0.8711782693862915, \n",
      "actor loss: 11.26909351348877, critic loss: 0.7849733233451843\n",
      "2024-10-04 01:23:14,557 global_step: 99,epoch: 4, kl_loss: 0.006596563022811802, reconst_loss: 0.027967847655622328, reward_loss: 0.11350071812951368, actor_loss: 11.26909351348877, critic_loss: 0.7849733233451843, \n",
      "collecting data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                 | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 64\n",
    "seq_len = 50\n",
    "\n",
    "world_episodes = 1\n",
    "update_step = 20\n",
    "\n",
    "seed_episodes = 5\n",
    "test_interval = 3\n",
    "save_interval = 20\n",
    "print(\"collecting seed data...\")\n",
    "collect_data(env,state_dim, transition_representation, agent, replay_buffer, seed_episodes, device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_score=collect_data(env,state_dim, transition_representation, agent, replay_buffer, world_episodes, device)\n",
    "    logger.log(epoch*update_step,train_score=train_score)\n",
    "\n",
    "    if len(replay_buffer) < batch_size*seq_len:\n",
    "        continue\n",
    "    \n",
    "    #train world model and actor, critic\n",
    "    for _ in range(update_step):\n",
    "        batch = replay_buffer.sample_seq(batch_size, seq_len)\n",
    "        kl_loss,reconst_loss, reward_loss, actor_loss, critic_loss=train(batch,state_dim,deterministic_dim, device, transition_representation, reward, observation, agent, value, model_optimizer, actor_optimizer, critic_optimizer)\n",
    "        logger.log(epoch*update_step+_,epoch=epoch, kl_loss=kl_loss, reconst_loss=reconst_loss, reward_loss=reward_loss, actor_loss=actor_loss, critic_loss=critic_loss)\n",
    "\n",
    "    if epoch % test_interval == 0:\n",
    "        test_score=collect_data(env,state_dim, transition_representation, agent, replay_buffer, world_episodes, device,training=False)\n",
    "        logger.log(epoch*update_step,test_score=test_score)\n",
    "    if epoch % save_interval == 0:\n",
    "        torch.save(transition_representation.state_dict(), 'transition_representation.pth')\n",
    "        torch.save(observation.state_dict(), 'observation.pth')\n",
    "        torch.save(reward.state_dict(), 'reward.pth')\n",
    "        torch.save(agent.state_dict(), 'agent.pth')\n",
    "        torch.save(value.state_dict(), 'value.pth')\n",
    "torch.save(transition_representation.state_dict(), 'transition_representation.pth')\n",
    "torch.save(observation.state_dict(), 'observation.pth')\n",
    "torch.save(reward.state_dict(), 'reward.pth')\n",
    "torch.save(agent.state_dict(), 'agent.pth')\n",
    "torch.save(value.state_dict(), 'value.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
